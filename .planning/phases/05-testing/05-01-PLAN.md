# 05-testing: End-to-End Verification

**Phase**: 05-testing
**Goal**: Verify full pipeline works and connect Opencode

## Context

@.planning/BRIEF.md - Project requirements
@.planning/ROADMAP.md - Phase overview
@.planning/phases/04-integration/04-01-PLAN.md - Integration complete

## Tasks

### Task 1: Full end-to-end test
- [ ] Start Ollama: `ollama serve`
- [ ] Start server: `uvicorn server:app --reload --port 5005`
- [ ] Test with curl:
```bash
curl -X POST http://localhost:5005/generate \
  -H "Content-Type: application/json" \
  -d '{"prompt": "What is Python?"}'
```
- [ ] Verify response contains relevant information

### Task 2: Test knowledge graph integration
- [ ] Send query about topic in knowledge graph
- [ ] Verify response is informed by graph context

### Task 3: Test with Opencode
- [ ] Connect Opencode to `http://localhost:5005/generate`
- [ ] Verify Opencode can make requests
- [ ] Test a simple prompt through Opencode

### Task 4: Verify success criteria
From BRIEF.md:
- [ ] Local LLM runs and responds
- [ ] FastAPI at `http://localhost:5005/generate`
- [ ] POST returns LLM responses
- [ ] Knowledge graph provides context
- [ ] Opencode connected to endpoint

## Verification

- [ ] All BRIEF.md success criteria met
- [ ] Server handles multiple requests
- [ ] No errors in server logs
- [ ] Response time is reasonable (< 30s)

## Notes

- Deadline is tomorrow - prioritize passing tests
- Document any issues in SUMMARY.md
- Can iterate after initial success
