# 02-llm-server: FastAPI + Ollama Integration

**Phase**: 02-llm-server
**Goal**: Create FastAPI server that wraps Ollama and hosts at port 5005

## Context

@.planning/BRIEF.md - Project requirements
@.planning/ROADMAP.md - Phase overview
@.planning/phases/01-foundation/01-01-PLAN.md - Foundation complete

## Tasks

### Task 1: Create server.py with FastAPI
- [ ] Import FastAPI, Uvicorn, requests
- [ ] Create `/generate` POST endpoint
- [ ] Accept JSON body: `{"prompt": "message"}`
- [ ] Forward prompt to Ollama at `http://localhost:11434/api/generate`
- [ ] Return Ollama response as JSON

### Task 2: Configure CORS
- [ ] Add CORS middleware to allow Opencode calls
- [ ] Allow all origins for development

### Task 3: Run server on port 5005
- [ ] Start server: `uvicorn server:app --reload --port 5005`
- [ ] Verify server starts without errors

### Task 4: Test API endpoint
- [ ] Send POST to `http://localhost:5005/generate`
- [ ] Body: `{"prompt": "What is Python?"}`
- [ ] Verify response returns

## Verification

- [ ] Server runs on port 5005
- [ ] `curl -X POST http://localhost:5005/generate -H "Content-Type: application/json" -d '{"prompt":"Hello"}'` returns response
- [ ] Response comes from TinyLlama via Ollama

## Notes

- Ollama API: POST to `/api/generate` with `{"model": "tinyllama", "prompt": "...", "stream": false}`
- Server forwards request to Ollama, returns response
