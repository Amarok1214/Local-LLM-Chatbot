# 01-foundation: Environment Setup

**Phase**: 01-foundation
**Goal**: Install Ollama, pull TinyLlama, set up project structure

## Context

@.planning/BRIEF.md - Project requirements
@.planning/ROADMAP.md - Phase overview

## Tasks

### Task 1: Install Ollama on Windows
- [ ] Download Ollama for Windows from https://ollama.com
- [ ] Install Ollama (default installer)
- [ ] Verify installation: `ollama --version`

### Task 2: Pull TinyLlama model
- [ ] Pull TinyLlama: `ollama pull tinyllama`
- [ ] Verify model is available: `ollama list`
- [ ] Test model works: `ollama run tinyllama "Hello"`

### Task 3: Set up project structure
- [ ] Create project files: `requirements.txt`, `server.py`
- [ ] Install dependencies: `pip install fastapi uvicorn requests networkx`
- [ ] Create basic FastAPI skeleton in `server.py`

### Task 4: Verify Ollama API
- [ ] Test Ollama is running: `curl http://localhost:11434/api/tags`
- [ ] Confirm API is accessible

## Verification

- [ ] `ollama --version` returns version
- [ ] `ollama list` shows tinyllama
- [ ] `requirements.txt` exists with fastapi, uvicorn, requests, networkx
- [ ] `server.py` has basic FastAPI import

## Notes

- Ollama runs on port 11434 by default
- TinyLlama is ~1GB, ideal for local development
- Windows installer may require admin rights
