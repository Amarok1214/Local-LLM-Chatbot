# 04-integration: Connect Graph Queries to LLM

**Phase**: 04-integration
**Goal**: Wire knowledge graph queries into the LLM prompt pipeline

## Context

@.planning/BRIEF.md - Project requirements
@.planning/ROADMAP.md - Phase overview
@.planning/phases/02-llm-server/02-01-PLAN.md - LLM server working
@.planning/phases/03-knowledge-graph/03-01-PLAN.md - Knowledge graph ready

## Tasks

### Task 1: Update server.py to use knowledge graph
- [ ] Import knowledge_graph module
- [ ] Modify `/generate` endpoint:
  1. Extract user prompt
  2. Query knowledge graph for relevant context
  3. Build enhanced prompt with context
  4. Send to Ollama
  5. Return response

### Task 2: Implement context augmentation
```python
# Pipeline:
# 1. user_message = "What is machine learning?"
# 2. context = query_knowledge_graph(user_message) 
# 3. enhanced_prompt = f"Context: {context}\n\nUser: {user_message}"
# 4. response = ollama.generate(enhanced_prompt)
```

### Task 3: Handle no-context cases
- [ ] If no relevant context found, use original prompt
- [ ] Don't break if graph query fails

### Task 4: Test integration
- [ ] Send query about known topic → should have context
- [ ] Send query about unknown topic → should still work without context

## Verification

- [ ] `/generate` endpoint now uses knowledge graph
- [ ] Known topics return context-enhanced responses
- [ ] Unknown topics still return LLM responses
- [ ] Server restarts successfully with new code

## Notes

- Mirrors prism's approach: query-first, then LLM
- Simple implementation: string concatenation for context
- Can improve with better retrieval later
